{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataset.AmazonFashionDataset import AmazonFashionDataset \n",
    "from model.SentimentClassifier import SentimentClassifier\n",
    "import yaml\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    data, target = zip(*batch)\n",
    "    # data = torch.tensor(data)\n",
    "    # print(data)\n",
    "    # exit()\n",
    "    data_lens = [len(x) for x in data]\n",
    "    data_padded = torch.nn.utils.rnn.pad_sequence(data, batch_first=True, padding_value=0)\n",
    "    print(data_padded)\n",
    "    print(data_lens)\n",
    "    #packed = torch.nn.utils.rnn.pack_padded_sequence(data_padded, data_lens, batch_first=True, enforce_sorted=False)\n",
    "    return data_padded, target, data_lens\n",
    "    print(packed)\n",
    "    exit()\n",
    "    data = [item[0] for item in batch]\n",
    "    target = [item[1] for item in batch]\n",
    "    #data = torch.LongTensor(data)\n",
    "\n",
    "    return batch #[data, target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1664,    86,    10,  2379,     1,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [   10,  2093,    58,    13,   469,  3053,     4,    13,  3650,    32,\n",
      "           156,  1741,     1,    10,  1145,  4565,    13,  7051,    46,   191,\n",
      "           520,  6534, 12569,   660,    16,    87,   375,    85,  5574,   285,\n",
      "            59,    13,   624,    55,   109,    48,   796,    78,   807,     1,\n",
      "           196,    78,   872,   389,    79,    37,    13,  2858,     4,    78,\n",
      "          4357,   119,     1]])\n",
      "[5, 53]\n",
      "AAAAAA\n",
      "PackedSequence(data=tensor([[ 0.0653, -0.0464, -0.1589,  ..., -0.1022,  0.1846,  0.0813],\n",
      "        [ 0.0111, -0.0622, -0.0921,  ..., -0.1116,  0.1237,  0.0521],\n",
      "        [ 0.0293, -0.0956, -0.1695,  ..., -0.2058,  0.2006,  0.0885],\n",
      "        ...,\n",
      "        [ 0.0360, -0.0573, -0.3293,  ..., -0.2157,  0.2230,  0.1695],\n",
      "        [ 0.0924, -0.0591, -0.3423,  ..., -0.2582,  0.2881,  0.2075],\n",
      "        [ 0.0040, -0.1242, -0.2291,  ..., -0.2113,  0.1915,  0.2820]],\n",
      "       grad_fn=<CatBackward0>), batch_sizes=tensor([2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1]), sorted_indices=tensor([1, 0]), unsorted_indices=tensor([1, 0]))\n",
      "tensor([ 0.0184, -0.1265, -0.2127,  0.0275,  0.0424,  0.0289,  0.2896, -0.0493,\n",
      "        -0.2271,  0.1543, -0.1790, -0.0335,  0.1660,  0.1264,  0.1559, -0.1675,\n",
      "        -0.1558, -0.1352,  0.1835,  0.2514, -0.1124, -0.0664,  0.1502,  0.1759,\n",
      "        -0.1713,  0.2979, -0.0789,  0.1167,  0.0092,  0.2192, -0.0424,  0.2415,\n",
      "        -0.0734, -0.0111,  0.0045,  0.0571,  0.0733, -0.0132,  0.0488,  0.0811,\n",
      "         0.0943, -0.0695,  0.0273,  0.1959,  0.1548, -0.1953, -0.2001, -0.0403,\n",
      "        -0.3150, -0.1157, -0.1557,  0.1657,  0.3928,  0.0151,  0.0379, -0.2005,\n",
      "         0.1568, -0.2450, -0.0430,  0.1611,  0.0082, -0.2118,  0.2095,  0.2483],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "slicer.shape torch.Size([2, 1])\n",
      "before slicing torch.Size([2, 53, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m i, (data, label, data_lens) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 16\u001b[0m     out \u001b[39m=\u001b[39m model(data, data_lens)\n\u001b[1;32m     17\u001b[0m     \u001b[39m#out = out.unsqueeze(0)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[39mprint\u001b[39m(out)\n",
      "File \u001b[0;32m~/Desktop/MLEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [11], line 30\u001b[0m, in \u001b[0;36mSentimentClassifier.forward\u001b[0;34m(self, x, x_len)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39m#lstm_actual_out = lstm_out_padded[:, slicer,:]\u001b[39;00m\n\u001b[1;32m     29\u001b[0m lstm_actual_out \u001b[39m=\u001b[39m lstm_out_padded\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m lstm_actual_out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mgather(lstm_out_padded[:], \u001b[39m0\u001b[39;49m, slicer)\n\u001b[1;32m     31\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mafter slicing\u001b[39m\u001b[39m\"\u001b[39m , lstm_actual_out\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     32\u001b[0m exit()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ],
   "source": [
    "yaml_name = \"cfg/train_config.yaml\"\n",
    "with open(yaml_name) as f:\n",
    "    yaml_file = open(yaml_name, 'r')\n",
    "cfg = yaml.load(yaml_file.read(), Loader=yaml.FullLoader)\n",
    "writer = SummaryWriter()\n",
    "dataset = AmazonFashionDataset(cfg)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, shuffle = False, batch_size=2, collate_fn = my_collate)\n",
    "\n",
    "model = SentimentClassifier(cfg[\"MODEL\"][\"EMBEDDING_SIZE\"], cfg[\"MODEL\"][\"HIDDEN_SIZE\"])\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "temp = 0\n",
    "for epoch in range(cfg[\"TRAIN\"][\"EPOCH\"]):\n",
    "    for i, (data, label, data_lens) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data, data_lens)\n",
    "        #out = out.unsqueeze(0)\n",
    "        print(out)\n",
    "        loss = criterion(out, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        writer.add_scalar(\"Loss/train\", loss, temp )\n",
    "        temp += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(torch.nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.embedding = torchtext.vocab.GloVe(name='twitter.27B', dim=25)\n",
    "        self.emdedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = torch.nn.LSTM(input_size = self.emdedding_size, hidden_size = self.hidden_size, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(self.hidden_size,64)\n",
    "        self.leaky_relu = torch.nn.LeakyReLU()\n",
    "        self.linear2 = torch.nn.Linear(64,3)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, x_len):\n",
    "        #x_embed = self.embedding.get_vecs_by_tokens(x)\n",
    "        print(\"AAAAAA\")\n",
    "\n",
    "        x_embed = self.embedding.vectors[x]\n",
    "        x_embed = torch.FloatTensor(x_embed)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(x_embed, x_len, batch_first=True, enforce_sorted=False)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(packed)\n",
    "        lstm_out_padded, lstm_output_lengths = torch.nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        print(lstm_out)\n",
    "        print(lstm_out_padded[0][4])\n",
    "        slicer = [i-1 for i in x_len]\n",
    "        slicer = torch.tensor([[4], [52]])\n",
    "        print(\"slicer.shape\" , slicer.shape)\n",
    "        print(\"before slicing\" , lstm_out_padded.shape)\n",
    "        #lstm_actual_out = lstm_out_padded[:, slicer,:]\n",
    "        lstm_actual_out = lstm_out_padded.permute(2,0,1)\n",
    "        lstm_actual_out = torch.gather(lstm_out_padded[:], 0, slicer)\n",
    "        print(\"after slicing\" , lstm_actual_out.shape)\n",
    "        exit()\n",
    "        y1 = self.linear(lstm_out[:, -1])\n",
    "        y2 = self.leaky_relu(y1)\n",
    "        y3 = self.linear2(y2)\n",
    "        y4 = self.softmax(y3)\n",
    "        return y4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = torchtext.vocab.GloVe(name='twitter.27B', dim=25)\n",
    "emdedding_size = 25\n",
    "hidden_size = 64\n",
    "lstm = torch.nn.LSTM(input_size = 25, hidden_size = 64, batch_first=True)\n",
    "linear = torch.nn.Linear(64,64)\n",
    "leaky_relu = torch.nn.LeakyReLU()\n",
    "linear2 = torch.nn.Linear(64,3)\n",
    "softmax = torch.nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([[ 0.0701,  0.1503, -0.1439,  ...,  0.0219,  0.0982, -0.0636],\n",
      "        [ 0.0750,  0.0616, -0.0741,  ..., -0.0333,  0.0171, -0.0309],\n",
      "        [ 0.0465,  0.1703, -0.2167,  ...,  0.0098, -0.0192, -0.0690],\n",
      "        ...,\n",
      "        [ 0.1160,  0.1616, -0.2381,  ...,  0.0278,  0.1881, -0.1064],\n",
      "        [ 0.0961,  0.2416, -0.2744,  ...,  0.0520,  0.1787, -0.0895],\n",
      "        [ 0.0526,  0.1784, -0.2569,  ...,  0.1340,  0.1090,  0.0153]],\n",
      "       grad_fn=<CatBackward0>), batch_sizes=tensor([2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1]), sorted_indices=tensor([1, 0]), unsorted_indices=tensor([1, 0]))\n",
      "tensor([ 0.0464,  0.1602, -0.2364,  0.1861, -0.0481, -0.0557, -0.1290, -0.0621,\n",
      "         0.0765, -0.0538, -0.0463,  0.1067, -0.0596,  0.0884,  0.2256,  0.0388,\n",
      "         0.2230,  0.0056, -0.1956, -0.1015, -0.0595,  0.0375, -0.0452,  0.0195,\n",
      "        -0.1902,  0.1154, -0.0398,  0.1251, -0.0880, -0.1680, -0.3067,  0.0676,\n",
      "         0.0506, -0.0269,  0.3212,  0.0867, -0.1190, -0.1213, -0.1276,  0.2491,\n",
      "        -0.0772, -0.3419,  0.2373,  0.0746, -0.2497,  0.0732, -0.1783, -0.0685,\n",
      "        -0.1159, -0.1303, -0.0933,  0.1037, -0.0619,  0.1693, -0.0059,  0.1578,\n",
      "        -0.0660,  0.2270,  0.1718,  0.0863,  0.0490,  0.1306,  0.0997,  0.0325],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "slicer.shape torch.Size([2, 1])\n",
      "before slicing torch.Size([2, 53, 64])\n",
      "after slicing torch.Size([2, 1, 64])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_embed = embedding.vectors[data]\n",
    "x_embed = torch.FloatTensor(x_embed)\n",
    "packed = torch.nn.utils.rnn.pack_padded_sequence(x_embed, data_lens, batch_first=True, enforce_sorted=False)\n",
    "lstm_out, (h_n, c_n) = lstm(packed)\n",
    "lstm_out_padded, lstm_output_lengths = torch.nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "print(lstm_out)\n",
    "print(lstm_out_padded[0][4])\n",
    "slicer = [i-1 for i in data_lens]\n",
    "slicer = torch.tensor([[4], [52]])\n",
    "print(\"slicer.shape\" , slicer.shape)\n",
    "print(\"before slicing\" , lstm_out_padded.shape)\n",
    "#lstm_actual_out = lstm_out_padded[:, slicer,:]\n",
    "#lstm_actual_out = lstm_out_padded.permute(2,0,1)\n",
    "lstm_actual_out = torch.gather(lstm_out_padded, 1, slicer2)\n",
    "print(\"after slicing\" , lstm_actual_out.shape)\n",
    "\n",
    "# y1 = self.linear(lstm_out[:, -1])\n",
    "# y2 = self.leaky_relu(y1)\n",
    "# y3 = self.linear2(y2)\n",
    "# y4 = self.softmax(y3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1) must match the existing size (2) at non-singleton dimension 1.  Target sizes: [2, 1, 64].  Tensor sizes: [2, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [41], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(lens\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      4\u001b[0m slicer2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m64\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m slicer2[:] \u001b[39m=\u001b[39m lens\n\u001b[1;32m      6\u001b[0m slicer \n\u001b[1;32m      7\u001b[0m exit()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1) must match the existing size (2) at non-singleton dimension 1.  Target sizes: [2, 1, 64].  Tensor sizes: [2, 1]"
     ]
    }
   ],
   "source": [
    "lens = torch.tensor([4,52])\n",
    "lens = lens.unsqueeze(1)\n",
    "print(lens.shape)\n",
    "slicer2 = torch.zeros((2,1,64))\n",
    "slicer2[:] = lens\n",
    "slicer \n",
    "exit()\n",
    "slicer2[0,0] = 4\n",
    "slicer2[1,0] = 52\n",
    "slicer2 = slicer2.type(torch.int64)\n",
    "slicer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0464,  0.1602, -0.2364,  0.1861, -0.0481, -0.0557, -0.1290, -0.0621,\n",
       "         0.0765, -0.0538, -0.0463,  0.1067, -0.0596,  0.0884,  0.2256,  0.0388,\n",
       "         0.2230,  0.0056, -0.1956, -0.1015, -0.0595,  0.0375, -0.0452,  0.0195,\n",
       "        -0.1902,  0.1154, -0.0398,  0.1251, -0.0880, -0.1680, -0.3067,  0.0676,\n",
       "         0.0506, -0.0269,  0.3212,  0.0867, -0.1190, -0.1213, -0.1276,  0.2491,\n",
       "        -0.0772, -0.3419,  0.2373,  0.0746, -0.2497,  0.0732, -0.1783, -0.0685,\n",
       "        -0.1159, -0.1303, -0.0933,  0.1037, -0.0619,  0.1693, -0.0059,  0.1578,\n",
       "        -0.0660,  0.2270,  0.1718,  0.0863,  0.0490,  0.1306,  0.0997,  0.0325],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out_padded[0,4,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_actual_out.squeeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8726/2960823341.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:480.)\n",
      "  lstm_out_padded.grad\n"
     ]
    }
   ],
   "source": [
    "lstm_out_padded.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "slicer  = torch.tensor([7,14,29]).unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "slicer = x_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "           6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "           6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "           6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6]],\n",
       "\n",
       "        [[13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
       "          13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
       "          13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
       "          13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]],\n",
       "\n",
       "        [[28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
       "          28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
       "          28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
       "          28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slicer.expand(slicer.shape[0],64).unsqueeze(1) - 1#.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLKernel",
   "language": "python",
   "name": "mlkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
